# LLM Provider Implementation Status

## ‚úÖ Completed

### Core Types (`src/llm/provider.rs`)
- ‚úÖ `LLMProvider` trait with all methods including:
  - `compact()` - history compression via OpenAI Responses API
  - `get_history()` - retrieve conversation history
- ‚úÖ `ProviderState` - token usage tracking
- ‚úÖ `ProviderConfig` - generation parameters with:
  - `max_tool_turns` - automatic tool call/result pruning (default: 3)
- ‚úÖ `Message`, `Role`, `ToolCall` - conversation types
- ‚úÖ `Tool`, `ToolResult` - function calling
- ‚úÖ `StreamChunk` - streaming response types
- ‚úÖ `LoopStep` - chat loop events
- ‚úÖ `ChatLoopHandle` - bidirectional communication
- ‚úÖ `ToolCallAssembler` - parallel tool call helper
- ‚úÖ `ProviderError` - comprehensive error types
- ‚úÖ All unit tests passing

### OpenAI Provider (`src/llm/openai.rs`) - **COMPLETE**
- ‚úÖ Full implementation with manual `reqwest` + SSE parsing
- ‚úÖ `chat()` with streaming support
- ‚úÖ `chat_loop()` with tool calling support
- ‚úÖ Tool calling with parallel execution
- ‚úÖ History management:
  - Automatic tool turn pruning (configurable via `max_tool_turns`)
  - Manual history compaction via Responses API
  - History retrieval with `get_history()`
- ‚úÖ Configuration management
- ‚úÖ State tracking with `Arc<RwLock<>>`
- ‚úÖ Comprehensive error handling
- ‚úÖ Support for GPT-5+, o1, and gpt-4o models
- ‚úÖ Example code:
  - `examples/openai_basic.rs` - basic chat
  - `examples/simple_agent.rs` - multi-turn tool calling
  - `examples/interactive_agent.rs` - interactive chat with detailed tool logging

### Anthropic Provider (`src/llm/anthropic.rs`) - **COMPLETE**
- ‚úÖ Full implementation with manual `reqwest` + SSE parsing (eventsource_stream)
- ‚úÖ `chat()` with streaming support
- ‚úÖ `chat_loop()` with tool calling support
- ‚úÖ Tool calling with parallel execution
- ‚úÖ History management:
  - Automatic tool turn pruning (configurable via `max_tool_turns`)
  - History retrieval with `get_history()`
- ‚úÖ Configuration management
- ‚úÖ State tracking with `Arc<RwLock<>>`
- ‚úÖ Comprehensive error handling
- ‚úÖ Support for Claude models:
  - claude-opus-4-5-20251101
  - claude-sonnet-4-5-20250929
  - claude-3-7-sonnet-20250219
  - claude-sonnet-4-20250514
  - claude-3-5-haiku-20241022
- ‚úÖ Streaming SSE event types:
  - `message_start`, `content_block_start`, `content_block_delta`
  - `content_block_stop`, `message_delta`, `message_stop`
- ‚úÖ Content delta types: `text_delta`, `thinking_delta`, `input_json_delta`
- ‚ö†Ô∏è Not yet implemented:
  - Prompt caching (returns `ProviderError::CachingNotSupported`)
  - `compact()` history compression

### Helper Functions (`src/llm/helpers.rs`)
- ‚úÖ `chat_loop_with_tools()` - high-level chat loop wrapper
- ‚úÖ `ChatLoopConfig` with builder pattern:
  - Tool executors registration
  - `on_content` - streaming content callback
  - `on_tool_calls` - tool call notification callback
  - `on_tool_results` - tool result notification callback (shows results before LLM response)
  - `on_thinking` - thinking content callback
  - `max_rounds` - loop iteration limit
- ‚úÖ `ChatLoopResponse` - aggregated response with usage stats

### Tool Registry (`src/llm/registry.rs`) - **NEW**
- ‚úÖ `ToolRegistry` - centralized tool management
- ‚úÖ `register()` - register individual tools
- ‚úÖ `register_all_builtin()` - register all built-in tools at once
- ‚úÖ `get_tools_for_llm()` - get tool definitions for LLM
- ‚úÖ `execute()` - execute tool calls with automatic result wrapping
- ‚úÖ `has_tool()` / `get()` - tool lookup
- ‚úÖ Full test coverage

### Loop Detector (`src/llm/loop_detector.rs`) - **NEW**
- ‚úÖ `LoopDetector` - prevent repetitive tool calling patterns
- ‚úÖ Exact duplicate detection (same tool + same arguments)
- ‚úÖ Pattern detection (A‚ÜíB‚ÜíA‚ÜíB oscillating patterns)
- ‚úÖ Configurable via `LoopDetectorConfig`:
  - `max_exact_duplicates` (default: 3)
  - `exact_window_size` (default: 10)
  - `enable_pattern_detection` (default: true)
  - `min_pattern_length` / `max_pattern_length` (default: 2-3)
  - `pattern_window_size` (default: 20)
- ‚úÖ Escalating actions: `Continue` ‚Üí `Warn` ‚Üí `Terminate`
- ‚úÖ Warning message generation for LLM feedback
- ‚úÖ Full test coverage

### Tools (`src/tools/`)
- ‚úÖ `ToolProvider` trait - unified interface for custom tools
  - `name()` - tool identifier
  - `brief()` - short description for token-efficient prompts
  - `full_description()` - detailed usage information
  - `parameters()` - JSON Schema for tool arguments
  - `execute()` - async execution with Result return
- ‚úÖ `all_tools()` - returns all built-in tools for registration

- ‚úÖ `BashTool` (`src/tools/bash.rs`) - shell command execution:
  - Platform-aware (PowerShell on Windows, bash on Linux/macOS)
  - Configurable timeout (default: 30s)
  - Custom working directory support
  - Comprehensive error context
  - **Detailed tool description** with platform info, usage notes, constraints

- ‚úÖ `EditorEditTool` (`src/tools/editor_edit.rs`) - **NEW** file editing:
  - Multiple operation modes:
    - Basic mode: `edits` array with `old_text`/`new_text` pairs
    - Extended mode: `operation` field with `anchor`/`content`
  - Operations: `replace`, `insert_before`, `insert_after`, `delete`, `append`, `prepend`
  - `replace_all` option for batch replacements
  - Exact string matching (no regex, whitespace-sensitive)
  - Uniqueness validation (anchor must appear exactly once, unless `replace_all`)
  - Full test coverage

## üöß In Progress

None currently

## üìã To Do

### Gemini Provider (`src/llm/gemini.rs`)
- [ ] Manual implementation with `reqwest`
- [ ] SSE parsing
- [ ] Streaming support
- [ ] Tool calling (Function Calling API)
- [ ] Context caching
- [ ] Chat loop implementation

### Anthropic Provider Enhancements
- [ ] Prompt caching (Anthropic-specific feature)
- [ ] Extended thinking mode support (budget_tokens configuration)
- [ ] `compact()` history compression

### Shared Utilities
- [ ] Common HTTP client configuration
- [ ] Retry logic with exponential backoff
- [ ] Rate limiting handling

### Testing
- [ ] Integration tests with mock responses
- [ ] Tool calling scenario tests
- [ ] Streaming tests
- [ ] Error handling tests

### Documentation
- [ ] API documentation
- [ ] Usage examples for each provider
- [ ] Tool calling examples
- [ ] Migration guide (if switching providers)

### CLI Integration
- [ ] Add LLM commands to `km-tools` CLI
- [ ] Interactive chat mode
- [ ] Provider selection
- [ ] Configuration management

## üîë Key Design Decisions

### 1. Thread-Safe State Access
- Uses `Arc<RwLock<>>` internally
- Methods use `&self` instead of `&mut self`
- `state()` returns cloned `ProviderState`
- `update_config()` uses closure for updates

### 2. Bidirectional Chat Loop
- `ChatLoopHandle` with channels
- Events flow via `Stream<LoopStep>`
- Tool results submitted via `submit_tool_results()`
- Background task handles API calls

### 3. Streaming by Default
- `chat()` returns `Stream<StreamChunk>`
- Progressive output for better UX
- Easy to collect into full response if needed

### 4. Provider-Specific Features
- Feature flags: `openai`, `anthropic`, `gemini`
- Optional dependencies based on features
- Granular control over compilation

### 5. Tool Provider Abstraction
- `ToolProvider` trait enables custom tools
- `ToolRegistry` manages tool lifecycle
- `all_tools()` provides built-in tool collection
- Separation of brief/full descriptions for token efficiency

### 6. Loop Detection
- Escalating response: Warn ‚Üí Warn ‚Üí Terminate
- Detects both exact duplicates and oscillating patterns
- Configurable thresholds and window sizes
- Warning messages guide LLM to try different approaches

## üìö Dependencies

### Core
- `tokio` - Async runtime
- `tokio-stream` - Stream utilities
- `async-trait` - Trait async methods
- `async-stream` - Stream macros
- `futures` - Stream trait
- `reqwest` - HTTP client
- `thiserror` - Error handling
- `serde` / `serde_json` - Serialization
- `eventsource-stream` - SSE parsing for Anthropic

### Provider-Specific
- `async-openai = "0.32"` with `chat-completion` feature
- (Gemini: will use `reqwest` directly)

## üéØ Next Steps

**Recommended order:**

1. **Implement Gemini provider** (manual)
   - Manual implementation with `reqwest`
   - SSE parsing for streaming
   - Add context caching
   - Tool calling (Function Calling API)

2. **Enhance Anthropic provider**
   - Implement prompt caching
   - Add extended thinking mode configuration
   - Implement `compact()` history compression

3. **Add more tools**
   - File read tool
   - HTTP request tool
   - Database query tool
   - Custom tool examples

4. **Write comprehensive tests**
   - Mock responses
   - Edge cases
   - Error scenarios
   - Tool calling scenarios

5. **CLI integration**
   - Interactive mode
   - Configuration
   - Examples

## üéì Lessons Learned

### Tool Description Best Practices
- **Be concise**: Don't list commands/functions LLM already knows (e.g., PowerShell cmdlets, bash commands)
- **Focus on behavior**: Explain what the tool does, constraints, return formats
- **Platform awareness**: Tell LLM current OS and shell, let it choose appropriate commands
- **Clear structure**: Use sections (PLATFORM INFO, USAGE NOTES, RETURN FORMAT, WHEN TO USE, CONSTRAINTS)
- **Example description length**: ~2000-2500 characters is sufficient for detailed tool specs

### History Management Strategies
1. **Automatic pruning**: Keep last N tool turns (default: 3) to prevent token overflow
2. **Manual compaction**: Use provider's Responses API to compress long conversations
3. **History retrieval**: Provide `get_history()` for inspection and debugging

### Callback Architecture
- **Progressive disclosure**: Show tool calls ‚Üí execution ‚Üí results ‚Üí LLM response
- **Multiple callbacks**: Separate callbacks for different events (tool_calls, tool_results, content, thinking)
- **Timing matters**: Tool results should appear BEFORE LLM processes them, not after response

### Loop Detection Design
- **Escalating severity**: First warn, then terminate - gives LLM a chance to self-correct
- **Multiple detection strategies**: Exact duplicates catch simple loops, pattern detection catches oscillation
- **Informative warnings**: Tell LLM specifically what it's doing wrong and suggest alternatives
- **Configurable thresholds**: Different use cases may need different sensitivity levels

## üìñ Sources

- [async-openai crate](https://crates.io/crates/async-openai)
- [async-openai documentation](https://docs.rs/async-openai)
- [async-openai GitHub](https://github.com/64bit/async-openai)
- [eventsource-stream crate](https://crates.io/crates/eventsource-stream)
- OpenAI API Documentation
- Anthropic API Documentation
- Google Gemini API Documentation
